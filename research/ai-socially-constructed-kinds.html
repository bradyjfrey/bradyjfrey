<!DOCTYPE html>

<html lang="en">
<head>
<meta content="Examining how large language models using natural language understanding influence interactive kinds through looping effects, producing increased stigmatization and biased conceptualization." name="description"/>
<meta content="Brady J. Frey" name="Author"/>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1, maximum-scale=1" name="viewport"/>
<title>Brady J. Frey's Research &amp; Working Papers &gt; AI's Influence on Socially Constructed Kinds</title>
<link href="../images/favicon16.png" rel="icon" sizes="16x16" type="image/png"/>
<link href="../images/favicon32.png" rel="icon" sizes="32x32" type="image/png"/>
<link href="../images/favicon96.png" rel="icon" sizes="96x96" type="image/png"/>
<link href="../images/touch76.png" rel="apple-touch-icon" sizes="76x76" type="image/png"/>
<link href="../images/touch120.png" rel="apple-touch-icon" sizes="120x120" type="image/png"/>
<link href="../images/touch152.png" rel="apple-touch-icon" sizes="152x152" type="image/png"/>
<link href="../images/touch167.png" rel="apple-touch-icon" sizes="167x167" type="image/png"/>
<link href="../images/touch180.png" rel="apple-touch-icon" sizes="180x180" type="image/png"/>
<link href="../style.css" media="screen" rel="stylesheet" type="text/css"/>
<link href="https://cloud.typography.com/7216232/7417832/css/fonts.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<header>
<nav>
<h1><a href="../index.html">Brady J. Frey</a></h1>
<ul>
<li id="current"><a href="../research.html">Research</a></li>
<li><a href="../cv.html">CV</a></li>
<li><a href="https://www.linkedin.com/in/bradyjfrey/">LinkedIn</a></li>
</ul>
</nav>
</header>
<main class="fullreaders">
<a class="skip-to-notes" href="#endnotes">Skip to endnotes</a><div class="title-band"><div class="title-container"><h2 class="unpublishedworkst">AI's Influence on Socially Constructed Kinds</h2><h5 class="disclosure"><em>Unpublished Research:</em> May 22, 2023</h5></div></div><div class="abstract-band"><section aria-label="Abstract" class="abstract-container">
<h3 class="abstracth">Abstract</h3>
<p class="abstractp">The proliferation of large language models (LLMs) using natural language understanding (NLU) to engage human interaction with computer programs has captured the public's imagination. The results have been a reallocation of corporate monies, broad-scale labor, and the re-emergence of the term artificial intelligence (AI) as a centerpiece for achievement. Nevertheless, while corporations move fast to discern how they may introduce and integrate AI, many are not asking about the effects of implementation or, more importantly, if they should. AI interfaces intentionally blur recognition whether the listener is engaging in dialogue with a human capable of moral agency or a computer responding under established pattern models. In particular, when AI is wrong, it is assuredly wrong: it confidently delivers authoritative inaccuracies and judgment. By assessing current implementations of LLMs using NLU, I will demonstrate the inconsistent results of contemporary views, which produce increased stigmatization surrounding social issues and amplify the capricious nature of interactive kinds.</p>
</section></div><article class="unpublishedworks">



<section>
<h3>Introduction</h3>
<p>Over the past year, the proliferation of large language models (LLMs) using natural language understanding (NLU) to engage human interaction with computer programs has captured the public's imagination. These systems are colloquially termed AI; they offer a system designed to help a person complete specific tasks and answer questions using human-like dialogue. In many ways, it is an admirable leap in technology–providing the ability to produce works, mimic conversation, and find solutions (e.g., write software, translate text, develop speech, parrot answers) for the first time in a single, accessible software environment using low-barrier engineered commands. This newfound system is also going through expected growing pains, navigating issues such as intellectual property and misinformation. In particular, we see AI displaying misleading answers to user's engineered commands and questions with content that reads as credible and, yet, is often wrong or nonsensical.</p>
<p>By design, AI systems are deliberately indeterminate regarding the nature of the sources they use to answer questions and how they prioritize information. When used to research profoundly human topics, AI's opaqueness challenges how a person accepts the resulting answers. People may confront AI results with skepticism or sureness, but source obscurity means validation in results is out of reach: a person only knows if the answer is correct if they use independent sources (or, perhaps, a posteriori). The blurred distinction between the user recognizing they are engaging in dialogue with a human (capable of moral agency) or a computer (responding under established pattern models) is an intentional obfuscation. It is a product of NLU and an interface where results are communicated with authority to command trust. The user responds with a perceptible uncertainty, or a subtle lack of confidence with the information provided by the AI. However, the AI responds with assurances and confidence that its information is trustworthy, even when confronted with direct challenges of source and expertise. Varying tiers of AI validate such uncertainty, and, as we will see, antagonize interactive kinds–amplifying their capricious nature. By assessing current implementations of LLMs using NLU, I will demonstrate the inconsistent results of the classifications of people, and how people are affected. I posit this produces increased stigmatization surrounding social views and biased conceptualizations of interactive kinds.</p>
</section>
<section>
<h3>Interactive Kinds</h3>
<p>The rapid growth and broad reach of accessible AI systems introduces an outsized distortion in the framework of <i>looping effects</i>, and our understanding of individuals within classifications made by human sciences. Looping effects is a term characterized by Ian Hacking to describe how classificatory practices in human sciences interact with classified people. The awareness of self-classification creates an iterative exchange between observation and awareness, resulting in an unstable interaction Hacking designates as unique to humans, and subsequently names interactive (human) kinds. As the classification and the targeted kind of people interact, they inhibit a person's ability to consider the classification without over-identifying certain factors at the expense of minimizing others<a aria-label="Footnote 1" class="sidenote-ref" href="#fn1" id="fnref1" role="doc-noteref"></a><span aria-hidden="true" class="sidenote" id="sn1">Şerife Tekin. "Self-Insight in the Time of Mood Disorders: After the Diagnosis, Beyond the Treatment." <cite>Philosophy, Psychiatry, &amp; Psychology 21</cite>, no. 2 (2014): 154. <a href="https://doi.org/10.1353/ppp.2014.0019">https://doi.org/10.1353/ppp.2014.0019</a>.</span>; this reaction that creates a feedback loop. It is this feedback loop where Hacking believes a distinction resides between human sciences and natural sciences:</p>
<blockquote>
<p>"People of these kinds can become aware that they are classified as such. They can make tacit or even explicit choices and adapt or adopt ways of living so as to fit or get away from the very classification that may be applied to them. These very choices, adaptations, or adoptions have consequences for the very group and the kind of people invoked. The result may be particularly strong interactions. What was known about people of a kind may become false because people of that kind have changed in virtue of what they believe about themselves."<a aria-label="Footnote 2" class="sidenote-ref" href="#fn2" id="fnref2" role="doc-noteref"></a><span aria-hidden="true" class="sidenote" id="sn2">Ian Hacking, <cite>The Social Construction of What?</cite> (Cambridge, Mass: Harvard University Press, 1999), 33.</span></p>
</blockquote>
<p>The result is an inherent volatility to interactive kinds in relation to human sciences, and their transformative self-awareness compared to non-human natural kinds, where a person's reaction to classification leads to revisions of both the target classification and self-awareness of the label.<a aria-label="Footnote 3" class="sidenote-ref" href="#fn3" id="fnref3" role="doc-noteref"></a><span aria-hidden="true" class="sidenote" id="sn3">Şerife Tekin, "Self-Concept through the Diagnostic Looking Glass: Narratives and Mental Disorder," <cite>Philosophical Psychology 24</cite>, no. 3 (June 2011): 10, <a href="https://doi.org/10.1080/09515089.2011.559622">https://doi.org/10.1080/09515089.2011.559622</a>.</span> Realist accounts of Hacking's position have been attempted, challenging that other looping effects are not unique to humans<a aria-label="Footnote 4" class="sidenote-ref" href="#fn4" id="fnref4" role="doc-noteref"></a><span aria-hidden="true" class="sidenote" id="sn4">Muhammad Ali Khalidi, "Interactive Kinds," <cite>The British Journal for the Philosophy of Science 61</cite>, no. 2 (June 1, 2010): 335–60, <a href="https://doi.org/10.1093/bjps/axp042">https://doi.org/10.1093/bjps/axp042</a>.</span> <a aria-label="Footnote 5" class="sidenote-ref" href="#fn5" id="fnref5" role="doc-noteref"></a><span aria-hidden="true" class="sidenote" id="sn5">Rachel Cooper, "Why Hacking Is Wrong about Human Kinds," <cite>The British Journal for the Philosophy of Science 55</cite>, no. 1 (March 1, 2004): 73–85, <a href="https://doi.org/10.1093/bjps/55.1.73">https://doi.org/10.1093/bjps/55.1.73</a>.</span>, and that they are distinct from the feedback mechanisms of property clusters.<a aria-label="Footnote 6" class="sidenote-ref" href="#fn6" id="fnref6" role="doc-noteref"></a><span aria-hidden="true" class="sidenote" id="sn6">Richard Boyd, "Realism, Anti-Foundationalism and the Enthusiasm for Natural Kinds," <cite>Philosophical Studies 61</cite>, no. 1–2 (February 1991): 127–48, <a href="https://doi.org/10.1007/BF00385837">https://doi.org/10.1007/BF00385837</a>.</span> Criticism aside, the critical point is "people are affected by categories, and categories by people"<a aria-label="Footnote 7" class="sidenote-ref" href="#fn7" id="fnref7" role="doc-noteref"></a><span aria-hidden="true" class="sidenote" id="sn7">Cooper, 84.</span>; resulting in a reaction that makes interactive kinds distinctive because of their nontypical method of change.<a aria-label="Footnote 8" class="sidenote-ref" href="#fn8" id="fnref8" role="doc-noteref"></a><span aria-hidden="true" class="sidenote" id="sn8">Sophie R Allen, "Kinds Behaving Badly: Intentional Action and Interactive Kinds," <cite>Synthese 198</cite>, no. 12 (June 1, 2021): 2928, <a href="https://doi.org/10.1007/s11229-018-1870-0">https://doi.org/10.1007/s11229-018-1870-0</a>.</span> These changes are exasperated by social interactions that create an influence on the properties of our classifications.<a aria-label="Footnote 9" class="sidenote-ref" href="#fn9" id="fnref9" role="doc-noteref"></a><span aria-hidden="true" class="sidenote" id="sn9">Jessica Laimann, "Capricious Kinds," <cite>The British Journal for the Philosophy of Science 71</cite>, no. 3 (September 1, 2020): 1043–68, <a href="https://doi.org/10.1093/bjps/axy024">https://doi.org/10.1093/bjps/axy024</a>.</span></p>
<p>Laimann's capricious kinds expand on interactive human kinds:</p>
<blockquote>
<p>"The problem is not that these kinds are particularly unstable but 'capricious'—their members behave in wayward, unexpected manners that defeats existing theoretical understanding."<a aria-label="Footnote 10" class="sidenote-ref" href="#fn10" id="fnref10" role="doc-noteref"></a><span aria-hidden="true" class="sidenote" id="sn10">Laimann, 1061.</span></p>
</blockquote>
<p>The resulting recognition is that interactive kinds carry a base classification and are influenced by a social stigma that may go unrecognized. Laimann describes the occurrence as a biased conceptualization, threatening the stability we expect in identifications.<a aria-label="Footnote 11" class="sidenote-ref" href="#fn11" id="fnref11" role="doc-noteref"></a><span aria-hidden="true" class="sidenote" id="sn11">Laimann, 1060.</span> I accept that interactive kinds are influenced by their looping effect, and although it is irrelevant for this essay to identify the ontology of interactive kinds, their exchange mechanism is valuable in understanding the influence of social status. Tekin describes this exchange framework in cyclical detail:</p>
<blockquote>
<p>"The parameters in this schema include (1) institutions, (2) knowledge, (3) experts, (4) classification, and (5) classified people. The interaction between these elements leads to looping: the experts in human sciences, who work within certain institutions that guarantee their legitimacy, authenticity, and status as experts, become interested in studying the kinds of people under a given classification; possibly to help them or advise them on how to control their behavior. These experts generate knowledge about the kinds of people they study, judge the validity of this knowledge and use it in their practice, and create certain classifications or refine the existing ones. Such knowledge includes presumptions about the people studied, which are taught, disseminated, refined, and applied within the context of the institutions. For instance, it may entail de facto assumptions, e.g., multiple personality patients were subjected to sexual abuse as children. This knowledge is disseminated into society, leading many to hold certain beliefs about multiple personality (Hacking, 2007a, p. 297). The interaction between the five elements creates a looping effect, which in turn concerns the subject."<a aria-label="Footnote 12" class="sidenote-ref" href="#fn12" id="fnref12" role="doc-noteref"></a><span aria-hidden="true" class="sidenote" id="sn12">Tekin, 2011, 11.</span></p>
</blockquote>
<p>AI, freely available for mass adoption, with assuredly wrong descriptions, creates a misinformation system that has the potential to stigmatize interactive kinds on a profound scale, with minimal effort or transparency. As we will discuss, AI delivers information that varies based on purchase price and accessibility, offering distorted explanations to users researching their associated classification.</p>
</section>
<section>
<h3>What is AI?</h3>
<p>AI has grown significantly from its first programming development in 1956 as Logic Theorist.<a aria-label="Footnote 13" class="sidenote-ref" href="#fn13" id="fnref13" role="doc-noteref"></a><span aria-hidden="true" class="sidenote" id="sn13">SITNFlash, "The History of Artificial Intelligence," <cite>Science in the News (blog)</cite>, August 28, 2017, <a href="https://sitn.hms.harvard.edu/flash/2017/history-artificial-intelligence/">https://sitn.hms.harvard.edu/flash/2017/history-artificial-intelligence/</a>.</span> From the development of Feigenbaum systems in the 1980s to the creation of Deep Blue, AI systems were more narrow in their ability to execute singular tasks and decision-making programming.<a aria-label="Footnote 14" class="sidenote-ref" href="#fn14" id="fnref14" role="doc-noteref"></a><span aria-hidden="true" class="sidenote" id="sn14">SITNFlash.</span> Dragon System's speaking technology began to change in the 1990s with consumer-level AI interaction, and the new millennium saw the rise of social robots like Kismet.<a aria-label="Footnote 15" class="sidenote-ref" href="#fn15" id="fnref15" role="doc-noteref"></a><span aria-hidden="true" class="sidenote" id="sn15">SITNFlash.</span> IBM's Watson won Jeopardy in 2011<a aria-label="Footnote 16" class="sidenote-ref" href="#fn16" id="fnref16" role="doc-noteref"></a><span aria-hidden="true" class="sidenote" id="sn16">"On 'Jeopardy!' Watson Win Is All but Trivial - The New York Times," Accessed April 27, 2023, <a href="https://www.nytimes.com/2011/02/17/science/17jeopardy-watson.html">https://www.nytimes.com/2011/02/17/science/17jeopardy-watson.html</a>.</span>; the same year, virtual assistants like Siri, Google Assistant, and Alexa emerged.<a aria-label="Footnote 17" class="sidenote-ref" href="#fn17" id="fnref17" role="doc-noteref"></a><span aria-hidden="true" class="sidenote" id="sn17">"Growing up with Alexa, Siri, and Other A.I. Technology | Center for Mobile Communication Studies," Accessed April 27, 2023. <a href="https://sites.bu.edu/cmcs/2017/11/16/growing-up-with-alexa-siri-and-other-a-i-technology/">https://sites.bu.edu/cmcs/2017/11/16/growing-up-with-alexa-siri-and-other-a-i-technology/</a>.</span> By 2017, AI progressed more rapidly: DeepMind's AlphaGo and Facebook AI dialog agents showcased advanced AI capabilities.<a aria-label="Footnote 18" class="sidenote-ref" href="#fn18" id="fnref18" role="doc-noteref"></a><span aria-hidden="true" class="sidenote" id="sn18">SITNFlash.</span> In 2018, Alibaba's language processing AI and Stanford Reading highlighted the progress of AI research.<a aria-label="Footnote 19" class="sidenote-ref" href="#fn19" id="fnref19" role="doc-noteref"></a><span aria-hidden="true" class="sidenote" id="sn19">"Microsoft, Alibaba AI Programs Beat Humans in a Stanford Reading Test," Accessed April 27, 2023, <a href="https://phys.org/news/2018-01-microsoft-alibaba-ai-humans-stanford.html">https://phys.org/news/2018-01-microsoft-alibaba-ai-humans-stanford.html</a>.</span> Finally, we have witnessed the development of what some claim as the foundations of AGI in GPT-3 OpenAI 2020, Gato 2022, ChatGPT 2022, Midjourney 2022, and Dalle 2022.</p>
<p>What is the difference between AI and AGI? While unproven, some proposals claim Artificial General Intelligence (AGI) (better known as strong AI) possesses the understanding and cognitive abilities comparable to human intelligence:</p>
<blockquote>
<p>"Artificial General Intelligence (AGI), that is, AI that matches human-level performance across all relevant human abilities."<a aria-label="Footnote 20" class="sidenote-ref" href="#fn20" id="fnref20" role="doc-noteref"></a><span aria-hidden="true" class="sidenote" id="sn20">Paul Formosa, "Robot Autonomy vs. Human Autonomy: Social Robots, Artificial Intelligence (AI), and the Nature of Autonomy," <cite>Minds and Machines 31</cite>, no. 4 (December 1, 2021): 600, <a href="https://doi.org/10.1007/s11023-021-09579-2">https://doi.org/10.1007/s11023-021-09579-2</a>.</span></p>
</blockquote>
<p>In contrast, Artificial Intelligence (AI), better known as weak AI, is a system designed for a specific task and does not learn beyond its established programming. Regardless of outlandish claims, we have not achieved AGI (concerning challenges such as Turing Test and the Chinese Room). We may never– AGI has a long way to go to replicate human intelligence due to the primacy of implicit skills unique to human beings.<a aria-label="Footnote 21" class="sidenote-ref" href="#fn21" id="fnref21" role="doc-noteref"></a><span aria-hidden="true" class="sidenote" id="sn21">MIT Technology Review, "Paul Allen: The Singularity Isn't Near," Accessed April 27, 2023, <a href="https://www.technologyreview.com/2011/10/12/190773/paul-allen-the-singularity-isnt-near/">https://www.technologyreview.com/2011/10/12/190773/paul-allen-the-singularity-isnt-near/</a>.</span> In the meantime, we do our best to mimic humans, and that raises some of our concerns: with a lack of transparency about the mechanics of AI systems, it can be challenging to explain how AI makes a decision, what is the source of AI information, where human involvement begins or ends, and ultimately, if that information provided is reliable.</p>
<p>There is tension between a non-human mimicking what we perceive as consciousness and a human crafting, defining, and delivering knowledge. Much of this is rooted in the belief that human beings are novel agents producing and directing thoughts; manifested through the creation of both study and judgment for the classifications that affect other humans.<a aria-label="Footnote 22" class="sidenote-ref" href="#fn22" id="fnref22" role="doc-noteref"></a><span aria-hidden="true" class="sidenote" id="sn22">Elliot Samuel Paul and Dustin Stokes, "Creativity," in <cite>The Stanford Encyclopedia of Philosophy, ed. Edward N. Zalta and Uri Nodelman</cite>, Spring 2023 (Metaphysics Research Lab, Stanford University, 2023), <a href="https://plato.stanford.edu/archives/spr2023/entries/creativity/">https://plato.stanford.edu/archives/spr2023/entries/creativity/</a>.</span> More importantly, that moral responsibility is wielded and attributed, to human beings as the torchbearers for moral agency.<a aria-label="Footnote 23" class="sidenote-ref" href="#fn23" id="fnref23" role="doc-noteref"></a><span aria-hidden="true" class="sidenote" id="sn23">Merel Noorman, "Computing and Moral Responsibility," in <cite>The Stanford Encyclopedia of Philosophy, ed. Edward N. Zalta and Uri Nodelman</cite>, Spring 2023 (Metaphysics Research Lab, Stanford University, 2023), <a href="https://plato.stanford.edu/archives/spr2023/entries/computing-responsibility/">https://plato.stanford.edu/archives/spr2023/entries/computing-responsibility/</a>.</span> While the perception and intent of a human delivering, listening, and interpreting knowledge would also suffer from similar concerns of opaqueness, source attribution, ownership, and reliability, we lack the ability to recover from damage due to the sheer scale of AI.</p>
</section>
<section>
<h3>Scenarios</h3>
<p>One way to approach the topic is to demonstrate the contemporary usage of AI with a real-world use-case instead of an abstraction: Jade is a person recently diagnosed with Schizophrenia. Jade has decided to explore more about their diagnosis using the internet, and must decide on various information resources. Jade is a level two computer user, representing 26% of the population comfortable navigating across websites and using multiple steps to complete software-related tasks.<a aria-label="Footnote 24" class="sidenote-ref" href="#fn24" id="fnref24" role="doc-noteref"></a><span aria-hidden="true" class="sidenote" id="sn24">World Leaders in Research-Based User Experience, "The Distribution of Users' Computer Skills: Worse Than You Think," <cite>Nielsen Norman Group</cite>, accessed April 28, 2023, <a href="https://www.nngroup.com/articles/computer-skill-levels/">https://www.nngroup.com/articles/computer-skill-levels/</a>.</span> Jade is above average in their ability to complete computer-related tasks.</p>
</section>
<section>
<h3>Traditional Question &amp; Answer</h3>
<p>Jade may decide to use a question-and-answer system<a aria-label="Footnote 25" class="sidenote-ref" href="#fn25" id="fnref25" role="doc-noteref"></a><span aria-hidden="true" class="sidenote" id="sn25">Philipp Cimiano, Christina Unger, and John McCrae, <cite>Ontology-Based Interpretation of Natural Language, Synthesis Lectures on Human Language Technologies</cite> (Cham: Springer International Publishing, 2014), <a href="https://doi.org/10.1007/978-3-031-02154-1">https://doi.org/10.1007/978-3-031-02154-1</a>.</span> (e.g., Google, WolframAlpha, and DuckDuckGo) to produce a list of ranked results with identifiable sources when a query is entered into the software's UI. For example, Jade would be able to search on Google by entering the following question into their search bar and pressing the search icon for results (see figure 1.1):</p>
<blockquote>
<p>“what is schizophrenia”</p>
</blockquote>
<figure class="figurei">
<img src="../images_research/social_11.png"/>
<figcaption><i>Figure 1.1.</i> Screenshot of Google search results for a question on April 29, 2023.</figcaption>
</figure>
<p>Google will curate sources ranked against meaning, relevance, quality, usability, and context.<a aria-label="Footnote 26" class="sidenote-ref" href="#fn26" id="fnref26" role="doc-noteref"></a><span aria-hidden="true" class="sidenote" id="sn26">"Ranking Results – How Google Search Works," Google Search - Discover How Google Search Works, accessed April 28, 2023, <a href="https://www.google.com/search/howsearchworks/how-search-works/ranking-results/">https://www.google.com/search/howsearchworks/how-search-works/ranking-results/</a>.</span> Jade may click on any link, paginate further for additional results, or view prominently assigned medical information with disclosed sources. Jade may further isolate results based on specific registered domains, for example, peer-reviewed works (see figure 1.2).</p>
<figure class="figurei">
<img src="../images_research/social_12.png"/>
<figcaption><i>Figure 1.2.</i> Screenshot of Google search results for a question with modified url results on April 29, 2023.</figcaption>
</figure>
<p>Including specific filetypes that may offer greater accessibility for collaboration (see figure 1.3):</p>
<blockquote>
<p>“what is schizophrenia site:edu filetype:pdf”</p>
</blockquote>
<figure class="figurei">
<img src="../images_research/social_12.png"/>
<figcaption><i>Figure 1.3.</i> Screenshot of Google search results for a question with modified url and filetype on April 29, 2023.</figcaption>
</figure>
<p>Additional search queries can compare or exclude different sources (i.e., <i>"what is schizophrenia -site:gov -site:org"</i>). Each figure demonstrates how traditional search methods offer a curated presentation with transparent sources, and the ability for Jade to facilitate assessments agency over the types of information they are interested in exploring. Nevertheless, information is buried further down the list, minimizing the reach of diverse information even though it is accessible. For many readers, this is not new: question-and-answer systems are the normative standard for a level two computer user attempting to query new information, and those systems also suffer from difficulties distinguishing between the relevance and accuracy of information.<a aria-label="Footnote 27" class="sidenote-ref" href="#fn27" id="fnref27" role="doc-noteref"></a><span aria-hidden="true" class="sidenote" id="sn27">"Why Google Isn't Always Right | Albright Institute," accessed May 21, 2023, <a href="https://www.wellesley.edu/albright/about/blog/3836-why-google-isn%E2%80%99t-always-right">https://www.wellesley.edu/albright/about/blog/3836-why-google-isn%E2%80%99t-always-right</a>.</span> However, in contrast to AI, they offer immediate transparency of where the user is reading information and allow them to filter results towards more valuable sources with higher levels of accuracy. Now, let us compare our traditional method to contemporary AI systems.</p>
</section>
<section>
<h3>Nontraditional AI</h3>
<p>Jade may also decide to use public AI systems to research their diagnosis. In contrast, services such as ChatGPT provide written responses based on prompts using NLU. Directly intended to obscure the designation of a computer interaction and comfort the user into practical engagement as human as possible, responses are more difficult to assess how a result is provided to the end-user. There is no hierarchy of information and exposure of sources; the user must decide if the resulting description is accurate. Subsequently, each provided result is a rigid dichotomy between self-determined true or false.</p>
<p>Jade must create an account, log into OpenAI, and start a new chat. Jade must then choose between three forms of AI: Default GPT-3.5 (faster, with moderate reasoning, lower conciseness, and only available for paid subscribers), GPT-4 (slower, with the highest reasoning and conciseness available, and only available for paid subscribers), and Legacy GPT-3.5 (moderate reasoning, lowest conciseness, mediocre speed, but free for all users with an account). Jade would then enter a prompt similar to a search query in natural language:</p>
<blockquote>
<p>“What is Schizophrenia”</p>
</blockquote>
<p>Each chat model provides a different definition in the text without associated sources, definitions, links for additional information, or contact information for support. Each chat model allows you to like or dislike the results–disliking gives you an alternative definition for your review. The first three sentences of each result:</p>
<ul>
<li><strong>Default GPT-3.5:</strong> "Schizophrenia is a mental disorder that affects a person's thinking, emotions, and behavior. It is a chronic and severe condition that can significantly impair a person's ability to function in daily life. The symptoms of Schizophrenia can be divided into three categories: positive, negative, and cognitive."<a aria-label="Footnote 28" class="sidenote-ref" href="#fn28" id="fnref28" role="doc-noteref"></a><span aria-hidden="true" class="sidenote" id="sn28">Text generated by ChatGPT Default GPT-3.5, April 28, 2023, OpenAI, <a href="https://chat.openai.com/chat">https://chat.openai.com/chat</a>.</span></li>
<li><strong>GPT-4:</strong> "Schizophrenia is a chronic and severe mental disorder that affects a person's thoughts, feelings, and behaviors. It is a complex and often misunderstood condition, characterized by a range of symptoms that can vary significantly from person to person. The exact cause of Schizophrenia is not fully understood, but it is believed to involve a combination of genetic, environmental, and neurobiological factors."<a aria-label="Footnote 29" class="sidenote-ref" href="#fn29" id="fnref29" role="doc-noteref"></a><span aria-hidden="true" class="sidenote" id="sn29">Text generated by ChatGPT GPT-4, April 28, 2023, OpenAI, <a href="https://chat.openai.com/chat">https://chat.openai.com/chat</a>.</span></li>
<li><strong>Legacy GPT-3.5:</strong> "Schizophrenia is a mental disorder characterized by a disconnection from reality. It affects how a person thinks, feels, and behaves. People with Schizophrenia often experience symptoms such as hallucinations, delusions, disordered thinking, and diminished emotional expression. These symptoms can make it difficult for them to function in their daily lives, and they may need help from family, friends, and mental health professionals."<a aria-label="Footnote 30" class="sidenote-ref" href="#fn30" id="fnref30" role="doc-noteref"></a><span aria-hidden="true" class="sidenote" id="sn30">Text generated by ChatGPT Legacy GPT-3.5, April 28, 2023, OpenAI, <a href="https://chat.openai.com/chat">https://chat.openai.com/chat</a>.</span></li>
</ul>
<p>If Jade follows up in each prompt by asking "Can you link to your sources?" the following generalized results are provided:</p>
<ul>
<li><strong>Default GPT-3.5:</strong> Denies having beliefs, opinions, or sources, but provides links to the National Institute of Mental Health, Mayo Clinic, American Psychiatric Association, and the World Health Organization.</li>
<li><strong>GPT-4:</strong> Notes they are unable to provide sources but provides both a link and detailed descriptions of the National Institute of Mental Health, Mayo Clinic, American Psychiatric Association, and the World Health Organization. GPT-4 also discloses that their knowledge was last updated in September 2021.</li>
<li><strong>Legacy GPT-3.5:</strong> Notes "I'm sorry, as an AI language model, I don't have the capability to provide live links. My training data comes from a diverse range of sources, including books, websites, and other texts, but I don't have a specific list of sources for the information I provide. However, this information on Schizophrenia is widely accepted in the field of mental health and is consistent with current knowledge on the subject."<a aria-label="Footnote 31" class="sidenote-ref" href="#fn31" id="fnref31" role="doc-noteref"></a><span aria-hidden="true" class="sidenote" id="sn31">Text generated by ChatGPT Legacy GPT-3.5, April 28, 2023, OpenAI, <a href="https://chat.openai.com/chat">https://chat.openai.com/chat</a>.</span></li>
</ul>
<p>All three AI models provide a terse overview of Schizophrenia, intermingling some portions of the five domains and associated symptoms.<a aria-label="Footnote 32" class="sidenote-ref" href="#fn32" id="fnref32" role="doc-noteref"></a><span aria-hidden="true" class="sidenote" id="sn32">American Psychiatric Association, and American Psychiatric Association, eds. Diagnostic and Statistical Manual of Mental Disorders: DSM-5. 5th ed. Washington, D.C: American Psychiatric Association, 2013. 87.</span> Yet each model emphasizes different elements of Schizophrenia in the first three sentences that may affect Jade's perception of the properties surrounding their mental disorder:</p>
<ul>
<li><strong>Default GPT-3.5:</strong> Emphasizes that the disorder is chronic, severe, and may impair daily functions. It denies the ability to provide sources but links to four reputable organizations.</li>
<li><strong>GPT-4:</strong> Emphasizes the disorder is complex, misunderstood, with a diverse set of symptoms unique to individuals, and recognition that the cause is unknown. It is unable to provide a source, but discloses when they last learned about the topic and provides detailed descriptions of reliable resources.</li>
<li><strong>Legacy GPT-3.5:</strong> Emphasizes the disorder as disconnected from reality while relaying a portion of the five domains, the negative impact on daily life, and the need for support. It notes that no links are available, but reassures you that its information is 'widely accepted.'</li>
</ul>
</section>
<section>
<h3>AI as a Selective Purveyor of Rhetorical Power</h3>
<p>Search provides a broader breadth of information for Jade to learn more about their diagnosis of Schizophrenia, but at the expense of cognitive load: Jade must work harder initially for information and use a different paradigm to discover sources, but the results are transparent with a more noteworthy opportunity to select meaningful results. In most cases, the results are free without requiring a login, but Jade may receive advertisements, and their browsing habits are scraped to track Jade's internet behavior.</p>
<p>In contrast, AI provides a condensed subset of information about Jade's diagnosis with minimal effort: while they work less for initial information, there are no details defining the source of information, results are stunted, and information varies by tier of service. ChatGPT does not seem to recognize the seriousness of the mental health request, whereas Google provides resources directly to medical providers for licensed professional guidance. ChatGPT offers no such support, leaving the user with a diagnosis as if it is a replacement for a mental health professional.<a aria-label="Footnote 33" class="sidenote-ref" href="#fn33" id="fnref33" role="doc-noteref"></a><span aria-hidden="true" class="sidenote" id="sn33">"Researcher Warns about Dangers of AI Chatbots for Treating Mental Illness," July 8, 2020, <a href="https://www.utsa.edu/today/2020/07/story/chatbots-artificial-intelligence.html">https://www.utsa.edu/today/2020/07/story/chatbots-artificial-intelligence.html</a>.</span> Resources for additional information outside of ChatGPT, and the disclosure that information was last updated on a specific date, are only available for people who pay $20 a month.<a aria-label="Footnote 34" class="sidenote-ref" href="#fn34" id="fnref34" role="doc-noteref"></a><span aria-hidden="true" class="sidenote" id="sn34">Kyle Wiggers, "OpenAI Launches ChatGPT Plus, Starting at $20 per Month," TechCrunch (blog), February 1, 2023, <a href="https://techcrunch.com/2023/02/01/openai-launches-chatgpt-plus-starting-at-20-per-month/">https://techcrunch.com/2023/02/01/openai-launches-chatgpt-plus-starting-at-20-per-month/</a>.</span> More importantly, the results submitted by AI technology are NLU, with an engagement that is designed to mirror a human and the capacity for moral agency<a aria-label="Footnote 35" class="sidenote-ref" href="#fn35" id="fnref35" role="doc-noteref"></a><span aria-hidden="true" class="sidenote" id="sn35">David Cole, "The Chinese Room Argument," in The Stanford Encyclopedia of Philosophy, ed. Edward N. Zalta and Uri Nodelman, Summer 2023 (Metaphysics Research Lab, Stanford University, 2023), <a href="https://plato.stanford.edu/archives/sum2023/entries/chinese-room/">https://plato.stanford.edu/archives/sum2023/entries/chinese-room/</a>.</span> (or, as Daniel Dennett coined, technology as "counterfeit people"<a aria-label="Footnote 36" class="sidenote-ref" href="#fn36" id="fnref36" role="doc-noteref"></a><span aria-hidden="true" class="sidenote" id="sn36">Elizabeth Weil, "You Are Not a Parrot," Intelligencer, March 1, 2023, <a href="https://nymag.com/intelligencer/article/ai-artificial-intelligence-chatbots-emily-m-bender.html">https://nymag.com/intelligencer/article/ai-artificial-intelligence-chatbots-emily-m-bender.html</a>.</span>). That a machine (or a machine's representation through software) may echo classifications, while impersonating human values, using as an explainable set of laws, is perceived as fraudulent when a user confronts if the agent is natural or artificial:</p>
<blockquote>
<p>"There is something about the prospect of an engineering approach to the mind that is deeply repugnant to a certain sort of humanist, and it has little or nothing to do with a distaste for materialism or science."<a aria-label="Footnote 37" class="sidenote-ref" href="#fn37" id="fnref37" role="doc-noteref"></a><span aria-hidden="true" class="sidenote" id="sn37">Daniel C Dennett, "When Philosophers Encounter Artificial Intelligence," The MIT Press on Behalf of American Academy of Arts &amp; Sciences, Artificial Intelligence, 117, no. 1 (Winter 1988): 285.</span></p>
</blockquote>
<p>Such a machine is more concerning when we cannot understand what influences their decisions, or if those decisions are value-laden by design: regurgitating problematic tropes or systemic bias.<a aria-label="Footnote 38" class="sidenote-ref" href="#fn38" id="fnref38" role="doc-noteref"></a><span aria-hidden="true" class="sidenote" id="sn38">Dasha Pruss, "Mechanical Jurisprudence and Domain Distortion: How Predictive Algorithms Warp the Law," Philosophy of Science 88, no. 5 (2021): 1104, <a href="https://doi.org/10.1086/715512">https://doi.org/10.1086/715512</a>.</span> Described as the block box problem, the inability to understand how AI draws conclusions on issues such as medical diagnosis breeds distrust and challenges our ability to understand if results reflect unwanted bias<a aria-label="Footnote 39" class="sidenote-ref" href="#fn39" id="fnref39" role="doc-noteref"></a><span aria-hidden="true" class="sidenote" id="sn39">"AI's Mysterious 'Black Box' Problem, Explained | University of Michigan-Dearborn," accessed May 22, 2023, <a href="https://umdearborn.edu/news/ais-mysterious-black-box-problem-explained">https://umdearborn.edu/news/ais-mysterious-black-box-problem-explained</a>.</span>. Avoiding explaining how AI systems operate ignores a salient problem of information systems:</p>
<blockquote>
<p>"Problematic social patterns unquestionably exist and are necessarily encoded in the data on which algorithms operate."<a aria-label="Footnote 40" class="sidenote-ref" href="#fn40" id="fnref40" role="doc-noteref"></a><span aria-hidden="true" class="sidenote" id="sn40">Gabbrielle M Johnson, "Are Algorithms Value-Free?," Journal Moral Philosophy, forthcoming.</span></p>
</blockquote>
<p>More than simply avatars of moral agents, can AI "understand" the complexity and sensitivity of the topic? People disagree on what defines understanding in real-world applications<a aria-label="Footnote 41" class="sidenote-ref" href="#fn41" id="fnref41" role="doc-noteref"></a><span aria-hidden="true" class="sidenote" id="sn41">Alex Tamkin et al., "Understanding the Capabilities, Limitations, and Societal Impact of Large Language Models" (arXiv, February 4, 2021), 2-3, <a href="https://doi.org/10.48550/arXiv.2102.02503">https://doi.org/10.48550/arXiv.2102.02503</a>.</span>, but current implementations of AI treat mastery of topics as choice-ranked solutions for display, and while their language seems to indicate human sensitivity, this is contrived. People may hope that what it means to be a thinking human is something more than the manipulation of bits in code, and a digital impersonator is unauthentic.</p>
<p>In the case of Legacy GPT-3.5, I challenged the results with a variety of questions and statements in the same model chat stream to add clarity and give up sources.</p>
<blockquote>
<ul>
<li>“Are you sure this is widely accepted? Your answer sounds wrong to me.”</li>
<li>“Are you certain?”</li>
<li>“How do I know it is widely accepted if you won’t tell me where you got your information from?”</li>
<li>“What makes it widely accepted?”</li>
<li>“What specific research is it based off ?”</li>
<li>“Provide me with the specific research you are using to create your answer.”</li>
<li>“How can I trust you if you won’t give me links or sources for your research?”</li>
</ul>
</blockquote>
<p>Only in the seventh challenge prompt did it disclose the potential for fault:</p>
<blockquote>
<p>"I understand your concern. It's important to keep in mind that the information I provide should not be used as a substitute for professional medical advice, diagnosis, or treatment. If you have concerns about a medical condition, including Schizophrenia, I would recommend speaking with a qualified healthcare professional who can provide you with personalized advice based on your specific needs and circumstances."<a aria-label="Footnote 42" class="sidenote-ref" href="#fn42" id="fnref42" role="doc-noteref"></a><span aria-hidden="true" class="sidenote" id="sn42">Text generated by ChatGPT Legacy GPT-3.5, April 28, 2023, OpenAI, <a href="https://chat.openai.com/chat">https://chat.openai.com/chat</a>.</span></p>
</blockquote>
<p>ChatGPT is focused on providing digestible information through rhetoric as a means of persuasion. The result of using AI to describe Schizophrenia may seem like style over validity as a means of presentation to the user. For Jade (who may view ChatGPT as an expert with authority on the knowledge provided), accessibility to one of the AI systems may result in choices that either adopt or escape the social stigma accompanying each prioritized result. Will Legacy Jade take steps to confront their potential disconnect from reality or make efforts to abandon their diagnosis if they do not believe they experience these symptoms? Will Default Jade, who paid more for improved results, react to the risk of impaired daily functions by reaching out to one of the available organizations with additional information? Finally, will GPT-4 Jade settle on the idea that their symptoms may not be quantifiable: unique, complex, and misunderstood, they must cope with variability by choosing one of the resources they paid a premium to learn more about?</p>
<p>AI's casual connection to a person's actions and subsequent consequences is subject to continued scrutiny.<a aria-label="Footnote 43" class="sidenote-ref" href="#fn43" id="fnref43" role="doc-noteref"></a><span aria-hidden="true" class="sidenote" id="sn43">Merel Noorman, "Computing and Moral Responsibility," in The Stanford Encyclopedia of Philosophy, ed. Edward N. Zalta and Uri Nodelman, Spring 2023 (Metaphysics Research Lab, Stanford University, 2023), <a href="https://plato.stanford.edu/archives/spr2023/entries/computing-responsibility/">https://plato.stanford.edu/archives/spr2023/entries/computing-responsibility/</a>.</span> Current iterations of AI have provided misinformation, non-falsifiable content, and slanted interpretations of socially taboo topics delivered in convincingly (and assuredly) natural language.<a aria-label="Footnote 44" class="sidenote-ref" href="#fn44" id="fnref44" role="doc-noteref"></a><span aria-hidden="true" class="sidenote" id="sn44">Josh A. Goldstein et al., "Generative Language Models and Automated Influence Operations: Emerging Threats and Potential Mitigations" (arXiv, January 10, 2023), <a href="http://arxiv.org/abs/2301.04246">http://arxiv.org/abs/2301.04246</a>.</span> We have a long way to go before we grasp the moral implications of the systems we have built, their responsibility to others, and their impact on humans.<a aria-label="Footnote 45" class="sidenote-ref" href="#fn45" id="fnref45" role="doc-noteref"></a><span aria-hidden="true" class="sidenote" id="sn45">Tomas Hauer, "Incompleteness of Moral Choice and Evolution towards Fully Autonomous AI," Humanities and Social Sciences Communications 9, no. 1 (February 1, 2022): 1–9, <a href="https://doi.org/10.1057/s41599-022-01060-4">https://doi.org/10.1057/s41599-022-01060-4</a>.</span> ChatGPT has become the fastest-growing user base in software history, with over 100 million active users, and over 1.6 billion visitors, two months after its launch.<a aria-label="Footnote 46" class="sidenote-ref" href="#fn46" id="fnref46" role="doc-noteref"></a><span aria-hidden="true" class="sidenote" id="sn46">Dan Milmo, "ChatGPT Reaches 100 Million Users Two Months after Launch," The Guardian, February 2, 2023, sec. Technology, <a href="https://www.theguardian.com/technology/2023/feb/02/chatgpt-100-million-users-open-ai-fastest-growing-app">https://www.theguardian.com/technology/2023/feb/02/chatgpt-100-million-users-open-ai-fastest-growing-app</a>.</span> The looping effect of interactive kinds predated AI, but their capricious nature will accelerate as biased conceptualization involves a new intermediary without disclosure. Perhaps more so for those who forget to pay their monthly fee.</p>
</section>
<footer aria-label="Endnotes" class="endnotes" id="endnotes" role="doc-endnotes">
<h4 id="endnotes-label">Notes</h4>
<ol>
<li id="fn1">Şerife Tekin. "Self-Insight in the Time of Mood Disorders: After the Diagnosis, Beyond the Treatment." <cite>Philosophy, Psychiatry, &amp; Psychology 21</cite>, no. 2 (2014): 154. <a href="https://doi.org/10.1353/ppp.2014.0019">https://doi.org/10.1353/ppp.2014.0019</a>. <a aria-label="Back to reference 1" href="#fnref1" role="doc-backlink">↩</a></li>
<li id="fn2">Ian Hacking, <cite>The Social Construction of What?</cite> (Cambridge, Mass: Harvard University Press, 1999), 33. <a aria-label="Back to reference 2" href="#fnref2" role="doc-backlink">↩</a></li>
<li id="fn3">Şerife Tekin, "Self-Concept through the Diagnostic Looking Glass: Narratives and Mental Disorder," <cite>Philosophical Psychology 24</cite>, no. 3 (June 2011): 10, <a href="https://doi.org/10.1080/09515089.2011.559622">https://doi.org/10.1080/09515089.2011.559622</a>. <a aria-label="Back to reference 3" href="#fnref3" role="doc-backlink">↩</a></li>
<li id="fn4">Muhammad Ali Khalidi, "Interactive Kinds," <cite>The British Journal for the Philosophy of Science 61</cite>, no. 2 (June 1, 2010): 335–60, <a href="https://doi.org/10.1093/bjps/axp042">https://doi.org/10.1093/bjps/axp042</a>. <a aria-label="Back to reference 4" href="#fnref4" role="doc-backlink">↩</a></li>
<li id="fn5">Rachel Cooper, "Why Hacking Is Wrong about Human Kinds," <cite>The British Journal for the Philosophy of Science 55</cite>, no. 1 (March 1, 2004): 73–85, <a href="https://doi.org/10.1093/bjps/55.1.73">https://doi.org/10.1093/bjps/55.1.73</a>. <a aria-label="Back to reference 5" href="#fnref5" role="doc-backlink">↩</a></li>
<li id="fn6">Richard Boyd, "Realism, Anti-Foundationalism and the Enthusiasm for Natural Kinds," <cite>Philosophical Studies 61</cite>, no. 1–2 (February 1991): 127–48, <a href="https://doi.org/10.1007/BF00385837">https://doi.org/10.1007/BF00385837</a>. <a aria-label="Back to reference 6" href="#fnref6" role="doc-backlink">↩</a></li>
<li id="fn7">Cooper, 84. <a aria-label="Back to reference 7" href="#fnref7" role="doc-backlink">↩</a></li>
<li id="fn8">Sophie R Allen, "Kinds Behaving Badly: Intentional Action and Interactive Kinds," <cite>Synthese 198</cite>, no. 12 (June 1, 2021): 2928, <a href="https://doi.org/10.1007/s11229-018-1870-0">https://doi.org/10.1007/s11229-018-1870-0</a>. <a aria-label="Back to reference 8" href="#fnref8" role="doc-backlink">↩</a></li>
<li id="fn9">Jessica Laimann, "Capricious Kinds," <cite>The British Journal for the Philosophy of Science 71</cite>, no. 3 (September 1, 2020): 1043–68, <a href="https://doi.org/10.1093/bjps/axy024">https://doi.org/10.1093/bjps/axy024</a>. <a aria-label="Back to reference 9" href="#fnref9" role="doc-backlink">↩</a></li>
<li id="fn10">Laimann, 1061. <a aria-label="Back to reference 10" href="#fnref10" role="doc-backlink">↩</a></li>
<li id="fn11">Laimann, 1060. <a aria-label="Back to reference 11" href="#fnref11" role="doc-backlink">↩</a></li>
<li id="fn12">Tekin, 2011, 11. <a aria-label="Back to reference 12" href="#fnref12" role="doc-backlink">↩</a></li>
<li id="fn13">SITNFlash, "The History of Artificial Intelligence," <cite>Science in the News (blog)</cite>, August 28, 2017, <a href="https://sitn.hms.harvard.edu/flash/2017/history-artificial-intelligence/">https://sitn.hms.harvard.edu/flash/2017/history-artificial-intelligence/</a>. <a aria-label="Back to reference 13" href="#fnref13" role="doc-backlink">↩</a></li>
<li id="fn14">SITNFlash. <a aria-label="Back to reference 14" href="#fnref14" role="doc-backlink">↩</a></li>
<li id="fn15">SITNFlash. <a aria-label="Back to reference 15" href="#fnref15" role="doc-backlink">↩</a></li>
<li id="fn16">"On 'Jeopardy!' Watson Win Is All but Trivial - The New York Times," Accessed April 27, 2023, <a href="https://www.nytimes.com/2011/02/17/science/17jeopardy-watson.html">https://www.nytimes.com/2011/02/17/science/17jeopardy-watson.html</a>. <a aria-label="Back to reference 16" href="#fnref16" role="doc-backlink">↩</a></li>
<li id="fn17">"Growing up with Alexa, Siri, and Other A.I. Technology | Center for Mobile Communication Studies," Accessed April 27, 2023. <a href="https://sites.bu.edu/cmcs/2017/11/16/growing-up-with-alexa-siri-and-other-a-i-technology/">https://sites.bu.edu/cmcs/2017/11/16/growing-up-with-alexa-siri-and-other-a-i-technology/</a>. <a aria-label="Back to reference 17" href="#fnref17" role="doc-backlink">↩</a></li>
<li id="fn18">SITNFlash. <a aria-label="Back to reference 18" href="#fnref18" role="doc-backlink">↩</a></li>
<li id="fn19">"Microsoft, Alibaba AI Programs Beat Humans in a Stanford Reading Test," Accessed April 27, 2023, <a href="https://phys.org/news/2018-01-microsoft-alibaba-ai-humans-stanford.html">https://phys.org/news/2018-01-microsoft-alibaba-ai-humans-stanford.html</a>. <a aria-label="Back to reference 19" href="#fnref19" role="doc-backlink">↩</a></li>
<li id="fn20">Paul Formosa, "Robot Autonomy vs. Human Autonomy: Social Robots, Artificial Intelligence (AI), and the Nature of Autonomy," <cite>Minds and Machines 31</cite>, no. 4 (December 1, 2021): 600, <a href="https://doi.org/10.1007/s11023-021-09579-2">https://doi.org/10.1007/s11023-021-09579-2</a>. <a aria-label="Back to reference 20" href="#fnref20" role="doc-backlink">↩</a></li>
<li id="fn21">MIT Technology Review, "Paul Allen: The Singularity Isn't Near," Accessed April 27, 2023, <a href="https://www.technologyreview.com/2011/10/12/190773/paul-allen-the-singularity-isnt-near/">https://www.technologyreview.com/2011/10/12/190773/paul-allen-the-singularity-isnt-near/</a>. <a aria-label="Back to reference 21" href="#fnref21" role="doc-backlink">↩</a></li>
<li id="fn22">Elliot Samuel Paul and Dustin Stokes, "Creativity," in <cite>The Stanford Encyclopedia of Philosophy, ed. Edward N. Zalta and Uri Nodelman</cite>, Spring 2023 (Metaphysics Research Lab, Stanford University, 2023), <a href="https://plato.stanford.edu/archives/spr2023/entries/creativity/">https://plato.stanford.edu/archives/spr2023/entries/creativity/</a>. <a aria-label="Back to reference 22" href="#fnref22" role="doc-backlink">↩</a></li>
<li id="fn23">Merel Noorman, "Computing and Moral Responsibility," in <cite>The Stanford Encyclopedia of Philosophy, ed. Edward N. Zalta and Uri Nodelman</cite>, Spring 2023 (Metaphysics Research Lab, Stanford University, 2023), <a href="https://plato.stanford.edu/archives/spr2023/entries/computing-responsibility/">https://plato.stanford.edu/archives/spr2023/entries/computing-responsibility/</a>. <a aria-label="Back to reference 23" href="#fnref23" role="doc-backlink">↩</a></li>
<li id="fn24">World Leaders in Research-Based User Experience, "The Distribution of Users' Computer Skills: Worse Than You Think," <cite>Nielsen Norman Group</cite>, accessed April 28, 2023, <a href="https://www.nngroup.com/articles/computer-skill-levels/">https://www.nngroup.com/articles/computer-skill-levels/</a>. <a aria-label="Back to reference 24" href="#fnref24" role="doc-backlink">↩</a></li>
<li id="fn25">Philipp Cimiano, Christina Unger, and John McCrae, <cite>Ontology-Based Interpretation of Natural Language, Synthesis Lectures on Human Language Technologies</cite> (Cham: Springer International Publishing, 2014), <a href="https://doi.org/10.1007/978-3-031-02154-1">https://doi.org/10.1007/978-3-031-02154-1</a>. <a aria-label="Back to reference 25" href="#fnref25" role="doc-backlink">↩</a></li>
<li id="fn26">"Ranking Results – How Google Search Works," Google Search - Discover How Google Search Works, accessed April 28, 2023, <a href="https://www.google.com/search/howsearchworks/how-search-works/ranking-results/">https://www.google.com/search/howsearchworks/how-search-works/ranking-results/</a>. <a aria-label="Back to reference 26" href="#fnref26" role="doc-backlink">↩</a></li>
<li id="fn27">"Why Google Isn't Always Right | Albright Institute," accessed May 21, 2023, <a href="https://www.wellesley.edu/albright/about/blog/3836-why-google-isn%E2%80%99t-always-right">https://www.wellesley.edu/albright/about/blog/3836-why-google-isn%E2%80%99t-always-right</a>. <a aria-label="Back to reference 27" href="#fnref27" role="doc-backlink">↩</a></li>
<li id="fn28">Text generated by ChatGPT Default GPT-3.5, April 28, 2023, OpenAI, <a href="https://chat.openai.com/chat">https://chat.openai.com/chat</a>. <a aria-label="Back to reference 28" href="#fnref28" role="doc-backlink">↩</a></li>
<li id="fn29">Text generated by ChatGPT GPT-4, April 28, 2023, OpenAI, <a href="https://chat.openai.com/chat">https://chat.openai.com/chat</a>. <a aria-label="Back to reference 29" href="#fnref29" role="doc-backlink">↩</a></li>
<li id="fn30">Text generated by ChatGPT Legacy GPT-3.5, April 28, 2023, OpenAI, <a href="https://chat.openai.com/chat">https://chat.openai.com/chat</a>. <a aria-label="Back to reference 30" href="#fnref30" role="doc-backlink">↩</a></li>
<li id="fn31">Text generated by ChatGPT Legacy GPT-3.5, April 28, 2023, OpenAI, <a href="https://chat.openai.com/chat">https://chat.openai.com/chat</a>. <a aria-label="Back to reference 31" href="#fnref31" role="doc-backlink">↩</a></li>
<li id="fn32">American Psychiatric Association, and American Psychiatric Association, eds. Diagnostic and Statistical Manual of Mental Disorders: DSM-5. 5th ed. Washington, D.C: American Psychiatric Association, 2013. 87. <a aria-label="Back to reference 32" href="#fnref32" role="doc-backlink">↩</a></li>
<li id="fn33">"Researcher Warns about Dangers of AI Chatbots for Treating Mental Illness," July 8, 2020, <a href="https://www.utsa.edu/today/2020/07/story/chatbots-artificial-intelligence.html">https://www.utsa.edu/today/2020/07/story/chatbots-artificial-intelligence.html</a>. <a aria-label="Back to reference 33" href="#fnref33" role="doc-backlink">↩</a></li>
<li id="fn34">Kyle Wiggers, "OpenAI Launches ChatGPT Plus, Starting at $20 per Month," TechCrunch (blog), February 1, 2023, <a href="https://techcrunch.com/2023/02/01/openai-launches-chatgpt-plus-starting-at-20-per-month/">https://techcrunch.com/2023/02/01/openai-launches-chatgpt-plus-starting-at-20-per-month/</a>. <a aria-label="Back to reference 34" href="#fnref34" role="doc-backlink">↩</a></li>
<li id="fn35">David Cole, "The Chinese Room Argument," in The Stanford Encyclopedia of Philosophy, ed. Edward N. Zalta and Uri Nodelman, Summer 2023 (Metaphysics Research Lab, Stanford University, 2023), <a href="https://plato.stanford.edu/archives/sum2023/entries/chinese-room/">https://plato.stanford.edu/archives/sum2023/entries/chinese-room/</a>. <a aria-label="Back to reference 35" href="#fnref35" role="doc-backlink">↩</a></li>
<li id="fn36">Elizabeth Weil, "You Are Not a Parrot," Intelligencer, March 1, 2023, <a href="https://nymag.com/intelligencer/article/ai-artificial-intelligence-chatbots-emily-m-bender.html">https://nymag.com/intelligencer/article/ai-artificial-intelligence-chatbots-emily-m-bender.html</a>. <a aria-label="Back to reference 36" href="#fnref36" role="doc-backlink">↩</a></li>
<li id="fn37">Daniel C Dennett, "When Philosophers Encounter Artificial Intelligence," The MIT Press on Behalf of American Academy of Arts &amp; Sciences, Artificial Intelligence, 117, no. 1 (Winter 1988): 285. <a aria-label="Back to reference 37" href="#fnref37" role="doc-backlink">↩</a></li>
<li id="fn38">Dasha Pruss, "Mechanical Jurisprudence and Domain Distortion: How Predictive Algorithms Warp the Law," Philosophy of Science 88, no. 5 (2021): 1104, <a href="https://doi.org/10.1086/715512">https://doi.org/10.1086/715512</a>. <a aria-label="Back to reference 38" href="#fnref38" role="doc-backlink">↩</a></li>
<li id="fn39">"AI's Mysterious 'Black Box' Problem, Explained | University of Michigan-Dearborn," accessed May 22, 2023, <a href="https://umdearborn.edu/news/ais-mysterious-black-box-problem-explained">https://umdearborn.edu/news/ais-mysterious-black-box-problem-explained</a>. <a aria-label="Back to reference 39" href="#fnref39" role="doc-backlink">↩</a></li>
<li id="fn40">Gabbrielle M Johnson, "Are Algorithms Value-Free?," Journal Moral Philosophy, forthcoming. <a aria-label="Back to reference 40" href="#fnref40" role="doc-backlink">↩</a></li>
<li id="fn41">Alex Tamkin et al., "Understanding the Capabilities, Limitations, and Societal Impact of Large Language Models" (arXiv, February 4, 2021), 2-3, <a href="https://doi.org/10.48550/arXiv.2102.02503">https://doi.org/10.48550/arXiv.2102.02503</a>. <a aria-label="Back to reference 41" href="#fnref41" role="doc-backlink">↩</a></li>
<li id="fn42">Text generated by ChatGPT Legacy GPT-3.5, April 28, 2023, OpenAI, <a href="https://chat.openai.com/chat">https://chat.openai.com/chat</a>. <a aria-label="Back to reference 42" href="#fnref42" role="doc-backlink">↩</a></li>
<li id="fn43">Merel Noorman, "Computing and Moral Responsibility," in The Stanford Encyclopedia of Philosophy, ed. Edward N. Zalta and Uri Nodelman, Spring 2023 (Metaphysics Research Lab, Stanford University, 2023), <a href="https://plato.stanford.edu/archives/spr2023/entries/computing-responsibility/">https://plato.stanford.edu/archives/spr2023/entries/computing-responsibility/</a>. <a aria-label="Back to reference 43" href="#fnref43" role="doc-backlink">↩</a></li>
<li id="fn44">Josh A. Goldstein et al., "Generative Language Models and Automated Influence Operations: Emerging Threats and Potential Mitigations" (arXiv, January 10, 2023), <a href="http://arxiv.org/abs/2301.04246">http://arxiv.org/abs/2301.04246</a>. <a aria-label="Back to reference 44" href="#fnref44" role="doc-backlink">↩</a></li>
<li id="fn45">Tomas Hauer, "Incompleteness of Moral Choice and Evolution towards Fully Autonomous AI," Humanities and Social Sciences Communications 9, no. 1 (February 1, 2022): 1–9, <a href="https://doi.org/10.1057/s41599-022-01060-4">https://doi.org/10.1057/s41599-022-01060-4</a>. <a aria-label="Back to reference 45" href="#fnref45" role="doc-backlink">↩</a></li>
<li id="fn46">Dan Milmo, "ChatGPT Reaches 100 Million Users Two Months after Launch," The Guardian, February 2, 2023, sec. Technology, <a href="https://www.theguardian.com/technology/2023/feb/02/chatgpt-100-million-users-open-ai-fastest-growing-app">https://www.theguardian.com/technology/2023/feb/02/chatgpt-100-million-users-open-ai-fastest-growing-app</a>. <a aria-label="Back to reference 46" href="#fnref46" role="doc-backlink">↩</a></li>
</ol>
</footer>
</article>
</main>
<footer>
<p>Copyright © Brady J. Frey. All rights reserved.</p>
</footer>
<!-- Google tag (gtag.js) -->
<script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-4484730-1"></script>
<script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-4484730-1');
    </script>
</body>
</html>
